# StockNow.ai ìŠ¤íƒ€ì¼ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬

ì‹¤ì‹œê°„ ê¸ˆìœµ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  ê´€ë ¨ ì¢…ëª©(Ticker)ì„ ìë™ ì¶”ì¶œí•˜ëŠ” í¬ë¡¤ëŸ¬ ì‹œìŠ¤í…œ

## ğŸ“‹ ëª©ì°¨
- [í”„ë¡œì íŠ¸ ê°œìš”](#í”„ë¡œì íŠ¸-ê°œìš”)
- [ì£¼ìš” ê¸°ëŠ¥](#ì£¼ìš”-ê¸°ëŠ¥)
- [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
- [ì„¤ì¹˜ ë°©ë²•](#ì„¤ì¹˜-ë°©ë²•)
- [ì‚¬ìš© ë°©ë²•](#ì‚¬ìš©-ë°©ë²•)
- [ë°ì´í„° êµ¬ì¡°](#ë°ì´í„°-êµ¬ì¡°)
- [êµ¬í˜„ ìƒì„¸](#êµ¬í˜„-ìƒì„¸)
- [API ë¬¸ì„œ](#api-ë¬¸ì„œ)

---

## ğŸ¯ í”„ë¡œì íŠ¸ ê°œìš”

StockNow.aiì™€ ìœ ì‚¬í•œ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” ê¸ˆìœµ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬:
- ì‹¤ì‹œê°„ ê¸ˆìœµ ë‰´ìŠ¤ ìˆ˜ì§‘
- ë‰´ìŠ¤ì—ì„œ ê´€ë ¨ ì¢…ëª© í‹°ì»¤ ìë™ ì¶”ì¶œ
- ì¢…ëª©ë³„/ì‹œê°„ë³„ ë‰´ìŠ¤ ë¶„ë¥˜
- ê°ì„± ë¶„ì„ ë° ì˜í–¥ë„ í‰ê°€

## âœ¨ ì£¼ìš” ê¸°ëŠ¥

### 1. ë‰´ìŠ¤ í¬ë¡¤ë§
- âœ… ë‹¤ì¤‘ ì†ŒìŠ¤ ì§€ì› (Bloomberg, Reuters, CNBC, WSJ, etc.)
- âœ… RSS í”¼ë“œ ë° HTML íŒŒì‹±
- âœ… ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ (5ë¶„ ê°„ê²©)
- âœ… ì¤‘ë³µ ì œê±° ë° ë°ì´í„° ì •ê·œí™”

### 2. í‹°ì»¤ ì¶”ì¶œ
- âœ… ëª…ì‹œì  í‹°ì»¤ ì¸ì‹ ($AAPL, (TSLA))
- âœ… íšŒì‚¬ëª… â†’ í‹°ì»¤ ë§¤í•‘
- âœ… NER ê¸°ë°˜ íšŒì‚¬ëª… ì¶”ì¶œ
- âœ… ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ê´€ë ¨ë„ ì ìˆ˜

### 3. ë°ì´í„° ë¶„ì„
- âœ… ê°ì„± ë¶„ì„ (ê¸ì •/ë¶€ì •/ì¤‘ë¦½)
- âœ… ë‰´ìŠ¤ ì¤‘ìš”ë„ í‰ê°€
- âœ… ì¢…ëª© ë©˜ì…˜ ë¹ˆë„ ì¶”ì 
- âœ… ì‹¤ì‹œê°„ íŠ¸ë Œë”© ì¢…ëª© ê°ì§€

### 4. API ì œê³µ
- âœ… RESTful API
- âœ… WebSocket ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°
- âœ… ì¢…ëª©ë³„/ì‹œê°„ë³„ í•„í„°ë§
- âœ… í˜ì´ì§€ë„¤ì´ì…˜

---

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  News Sources   â”‚
â”‚ (RSS/Web/API)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  News Crawler   â”‚
â”‚  (Scrapy/BS4)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ticker Extractorâ”‚
â”‚  (NER/Regex)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Sentiment       â”‚
â”‚ Analyzer        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Database      â”‚
â”‚ (PostgreSQL)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   REST API      â”‚
â”‚  (FastAPI)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ ì„¤ì¹˜ ë°©ë²•

### ìš”êµ¬ì‚¬í•­
- Python 3.9+
- PostgreSQL 14+
- Redis (ì„ íƒì‚¬í•­, ìºì‹±ìš©)

### ì„¤ì¹˜ ë‹¨ê³„

```bash
# 1. ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/yourusername/stocknow-crawler.git
cd stocknow-crawler

# 2. ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# 4. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
cp .env.example .env
# .env íŒŒì¼ í¸ì§‘í•˜ì—¬ API í‚¤ ë“± ì„¤ì •

# 5. ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
python scripts/init_db.py

# 6. í‹°ì»¤ ë§¤í•‘ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
python scripts/download_ticker_data.py
```

### requirements.txt
```txt
# í¬ë¡¤ë§
scrapy>=2.11.0
beautifulsoup4>=4.12.0
newspaper3k>=0.2.8
feedparser>=6.0.10
selenium>=4.15.0

# NLP & í…ìŠ¤íŠ¸ ë¶„ì„
spacy>=3.7.0
transformers>=4.35.0
torch>=2.1.0
nltk>=3.8.1

# ë°ì´í„° ì²˜ë¦¬
pandas>=2.1.0
numpy>=1.26.0

# ë°ì´í„°ë² ì´ìŠ¤
psycopg2-binary>=2.9.9
sqlalchemy>=2.0.23
alembic>=1.12.1

# API
fastapi>=0.104.0
uvicorn>=0.24.0
pydantic>=2.5.0

# ìœ í‹¸ë¦¬í‹°
python-dotenv>=1.0.0
requests>=2.31.0
aiohttp>=3.9.0
redis>=5.0.0
celery>=5.3.4

# ê¸ˆìœµ ë°ì´í„°
yfinance>=0.2.32
```

---

## ğŸ’» ì‚¬ìš© ë°©ë²•

### 1. í¬ë¡¤ëŸ¬ ì‹¤í–‰

```bash
# ì „ì²´ í¬ë¡¤ëŸ¬ ì‹¤í–‰
python run_crawler.py

# íŠ¹ì • ì†ŒìŠ¤ë§Œ í¬ë¡¤ë§
python run_crawler.py --sources bloomberg,reuters

# íŠ¹ì • ì¢…ëª©ë§Œ ëª¨ë‹ˆí„°ë§
python run_crawler.py --tickers AAPL,TSLA,NVDA
```

### 2. API ì„œë²„ ì‹¤í–‰

```bash
# ê°œë°œ ëª¨ë“œ
uvicorn app.main:app --reload --port 8000

# í”„ë¡œë•ì…˜ ëª¨ë“œ
gunicorn app.main:app -w 4 -k uvicorn.workers.UvicornWorker
```

### 3. Python ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì‚¬ìš©

```python
from stocknow_crawler import NewsCrawler, TickerExtractor

# í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
crawler = NewsCrawler(sources=['bloomberg', 'reuters'])

# ë‰´ìŠ¤ ìˆ˜ì§‘
news_items = crawler.fetch_latest(hours=24)

# í‹°ì»¤ ì¶”ì¶œ
extractor = TickerExtractor()
for news in news_items:
    tickers = extractor.extract(news['text'])
    news['tickers'] = tickers
```

---

## ğŸ“Š ë°ì´í„° êµ¬ì¡°

### ë‰´ìŠ¤ ì•„ì´í…œ ìŠ¤í‚¤ë§ˆ

```json
{
  "id": "uuid-string",
  "url": "https://example.com/article",
  "title": "Apple announces new iPhone",
  "summary": "Apple Inc. unveiled...",
  "content": "Full article text...",
  "source": "bloomberg",
  "author": "John Doe",
  "published_at": "2025-10-22T10:30:00Z",
  "crawled_at": "2025-10-22T10:35:00Z",
  "tickers": [
    {
      "symbol": "AAPL",
      "name": "Apple Inc.",
      "exchange": "NASDAQ",
      "confidence": 0.95,
      "mention_count": 5
    }
  ],
  "sentiment": {
    "score": 0.75,
    "label": "positive",
    "confidence": 0.88
  },
  "keywords": ["iphone", "apple", "technology"],
  "category": "technology",
  "importance_score": 8.5
}
```

### ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ

```sql
-- ë‰´ìŠ¤ í…Œì´ë¸”
CREATE TABLE news_articles (
    id UUID PRIMARY KEY,
    url TEXT UNIQUE NOT NULL,
    title TEXT NOT NULL,
    summary TEXT,
    content TEXT,
    source VARCHAR(50),
    author VARCHAR(255),
    published_at TIMESTAMP,
    crawled_at TIMESTAMP DEFAULT NOW(),
    sentiment_score FLOAT,
    sentiment_label VARCHAR(20),
    importance_score FLOAT,
    category VARCHAR(50),
    created_at TIMESTAMP DEFAULT NOW()
);

-- í‹°ì»¤ í…Œì´ë¸”
CREATE TABLE tickers (
    symbol VARCHAR(10) PRIMARY KEY,
    name TEXT NOT NULL,
    exchange VARCHAR(20),
    sector VARCHAR(50),
    industry VARCHAR(100)
);

-- ë‰´ìŠ¤-í‹°ì»¤ ê´€ê³„ í…Œì´ë¸”
CREATE TABLE news_tickers (
    id SERIAL PRIMARY KEY,
    news_id UUID REFERENCES news_articles(id),
    ticker_symbol VARCHAR(10) REFERENCES tickers(symbol),
    confidence FLOAT,
    mention_count INT,
    created_at TIMESTAMP DEFAULT NOW(),
    UNIQUE(news_id, ticker_symbol)
);

-- ì¸ë±ìŠ¤
CREATE INDEX idx_published_at ON news_articles(published_at DESC);
CREATE INDEX idx_ticker_symbol ON news_tickers(ticker_symbol);
CREATE INDEX idx_sentiment ON news_articles(sentiment_score);
```

---

## ğŸ”§ êµ¬í˜„ ìƒì„¸

### 1. ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ (`crawler/news_crawler.py`)

```python
import feedparser
from newspaper import Article
from typing import List, Dict

class NewsCrawler:
    def __init__(self, sources: List[str]):
        self.sources = sources
        self.rss_feeds = {
            'bloomberg': 'https://www.bloomberg.com/feeds/...',
            'reuters': 'https://www.reuters.com/rssfeed/...',
            'cnbc': 'https://www.cnbc.com/id/.../device/rss/',
        }
    
    def fetch_rss(self, source: str) -> List[Dict]:
        """RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        feed_url = self.rss_feeds.get(source)
        feed = feedparser.parse(feed_url)
        
        articles = []
        for entry in feed.entries:
            article = {
                'url': entry.link,
                'title': entry.title,
                'summary': entry.get('summary', ''),
                'published_at': entry.get('published', ''),
                'source': source
            }
            articles.append(article)
        
        return articles
    
    def extract_full_content(self, url: str) -> str:
        """ì „ì²´ ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ"""
        article = Article(url)
        article.download()
        article.parse()
        return article.text
```

### 2. í‹°ì»¤ ì¶”ì¶œê¸° (`extractor/ticker_extractor.py`)

```python
import re
import spacy
from typing import List, Dict, Set
import yfinance as yf

class TickerExtractor:
    def __init__(self):
        self.nlp = spacy.load("en_core_web_sm")
        self.ticker_db = self._load_ticker_database()
        self.company_to_ticker = self._build_company_map()
    
    def extract(self, text: str) -> List[Dict]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‹°ì»¤ ì¶”ì¶œ"""
        tickers = set()
        
        # 1. ëª…ì‹œì  í‹°ì»¤ íŒ¨í„´
        explicit = self._extract_explicit_tickers(text)
        tickers.update(explicit)
        
        # 2. íšŒì‚¬ëª…ì—ì„œ ì¶”ì¶œ
        companies = self._extract_companies(text)
        for company in companies:
            ticker = self.company_to_ticker.get(company.lower())
            if ticker:
                tickers.add(ticker)
        
        # 3. ê²€ì¦ ë° ìƒì„¸ ì •ë³´ ì¶”ê°€
        result = []
        for ticker in tickers:
            info = self._get_ticker_info(ticker)
            if info:
                info['mention_count'] = text.upper().count(ticker)
                result.append(info)
        
        return result
    
    def _extract_explicit_tickers(self, text: str) -> Set[str]:
        """ëª…ì‹œì  í‹°ì»¤ íŒ¨í„´ ì¶”ì¶œ"""
        patterns = [
            r'\$([A-Z]{1,5})\b',  # $AAPL
            r'\(([A-Z]{2,5})\)',   # (AAPL)
            r'(?:NYSE|NASDAQ):([A-Z]{1,5})',  # NASDAQ:AAPL
        ]
        
        tickers = set()
        for pattern in patterns:
            matches = re.findall(pattern, text)
            # ì˜ëª»ëœ ë§¤ì¹­ í•„í„°ë§
            valid = [m for m in matches if m not in 
                    ['USA', 'UK', 'EU', 'CEO', 'CFO']]
            tickers.update(valid)
        
        return tickers
    
    def _extract_companies(self, text: str) -> List[str]:
        """NERë¡œ íšŒì‚¬ëª… ì¶”ì¶œ"""
        doc = self.nlp(text)
        companies = [ent.text for ent in doc.ents 
                    if ent.label_ == "ORG"]
        return companies
    
    def _get_ticker_info(self, symbol: str) -> Dict:
        """í‹°ì»¤ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        try:
            ticker = yf.Ticker(symbol)
            info = ticker.info
            return {
                'symbol': symbol,
                'name': info.get('longName', ''),
                'exchange': info.get('exchange', ''),
                'confidence': 0.9  # ê²€ì¦ëœ í‹°ì»¤
            }
        except:
            return None
```

### 3. ê°ì„± ë¶„ì„ê¸° (`analyzer/sentiment_analyzer.py`)

```python
from transformers import pipeline
from typing import Dict

class SentimentAnalyzer:
    def __init__(self):
        # ê¸ˆìœµ íŠ¹í™” ëª¨ë¸ ì‚¬ìš©
        self.model = pipeline(
            "sentiment-analysis",
            model="ProsusAI/finbert"
        )
    
    def analyze(self, text: str) -> Dict:
        """í…ìŠ¤íŠ¸ ê°ì„± ë¶„ì„"""
        result = self.model(text[:512])[0]  # í† í° ì œí•œ
        
        return {
            'label': result['label'].lower(),
            'score': result['score'],
            'confidence': result['score']
        }
    
    def analyze_ticker_context(self, text: str, ticker: str) -> Dict:
        """íŠ¹ì • í‹°ì»¤ì— ëŒ€í•œ ì»¨í…ìŠ¤íŠ¸ ê°ì„± ë¶„ì„"""
        # í‹°ì»¤ ì£¼ë³€ ë¬¸ì¥ ì¶”ì¶œ
        sentences = text.split('.')
        relevant = [s for s in sentences if ticker in s.upper()]
        
        if not relevant:
            return self.analyze(text)
        
        context = '. '.join(relevant)
        return self.analyze(context)
```

### 4. FastAPI ì„œë²„ (`app/main.py`)

```python
from fastapi import FastAPI, Query, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from typing import List, Optional
from datetime import datetime, timedelta
import uvicorn

app = FastAPI(title="StockNow API", version="1.0.0")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/api/news")
async def get_news(
    tickers: Optional[str] = Query(None),
    hours: int = Query(24, ge=1, le=168),
    limit: int = Query(50, ge=1, le=200),
    sentiment: Optional[str] = Query(None),
):
    """ë‰´ìŠ¤ ì¡°íšŒ API"""
    # ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬
    query = NewsArticle.query
    
    # ì‹œê°„ í•„í„°
    start_time = datetime.utcnow() - timedelta(hours=hours)
    query = query.filter(NewsArticle.published_at >= start_time)
    
    # í‹°ì»¤ í•„í„°
    if tickers:
        ticker_list = tickers.split(',')
        query = query.join(NewsTicker).filter(
            NewsTicker.ticker_symbol.in_(ticker_list)
        )
    
    # ê°ì„± í•„í„°
    if sentiment:
        query = query.filter(NewsArticle.sentiment_label == sentiment)
    
    # ì •ë ¬ ë° ì œí•œ
    results = query.order_by(
        NewsArticle.published_at.desc()
    ).limit(limit).all()
    
    return [article.to_dict() for article in results]

@app.get("/api/tickers/{symbol}/news")
async def get_ticker_news(
    symbol: str,
    hours: int = Query(24, ge=1, le=168)
):
    """íŠ¹ì • ì¢…ëª©ì˜ ë‰´ìŠ¤ ì¡°íšŒ"""
    # êµ¬í˜„...
    pass

@app.get("/api/trending")
async def get_trending_tickers(
    hours: int = Query(24, ge=1, le=168),
    limit: int = Query(10, ge=1, le=50)
):
    """íŠ¸ë Œë”© ì¢…ëª© ì¡°íšŒ"""
    # êµ¬í˜„...
    pass

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

---

## ğŸ“¡ API ë¬¸ì„œ

### ì—”ë“œí¬ì¸íŠ¸

#### `GET /api/news`
ìµœì‹  ë‰´ìŠ¤ ì¡°íšŒ

**Query Parameters:**
- `tickers` (string, optional): ì‰¼í‘œë¡œ êµ¬ë¶„ëœ í‹°ì»¤ ëª©ë¡
- `hours` (int, default: 24): ì¡°íšŒ ê¸°ê°„ (ì‹œê°„)
- `limit` (int, default: 50): ê²°ê³¼ ê°œìˆ˜ ì œí•œ
- `sentiment` (string, optional): ê°ì„± í•„í„° (positive/negative/neutral)

**Response:**
```json
[
  {
    "id": "...",
    "title": "...",
    "tickers": [...],
    "sentiment": {...}
  }
]
```

#### `GET /api/tickers/{symbol}/news`
íŠ¹ì • ì¢…ëª© ë‰´ìŠ¤ ì¡°íšŒ

#### `GET /api/trending`
íŠ¸ë Œë”© ì¢…ëª© ì¡°íšŒ

#### `WebSocket /ws/news`
ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìŠ¤íŠ¸ë¦¼

---

## ğŸ” ë°ì´í„° ì†ŒìŠ¤

### ì§€ì› ë‰´ìŠ¤ ì†ŒìŠ¤
- Bloomberg
- Reuters
- CNBC
- Wall Street Journal
- Financial Times
- MarketWatch
- Seeking Alpha
- Yahoo Finance

### RSS í”¼ë“œ URL
```python
RSS_FEEDS = {
    'bloomberg': 'https://www.bloomberg.com/feeds/...',
    'reuters': 'https://www.reuters.com/rssfeed/...',
    'cnbc': 'https://www.cnbc.com/id/.../device/rss/',
    # ... ë” ë§ì€ ì†ŒìŠ¤
}
```

---

## âš™ï¸ ì„¤ì •

### í™˜ê²½ ë³€ìˆ˜ (.env)

```bash
# ë°ì´í„°ë² ì´ìŠ¤
DATABASE_URL=postgresql://user:password@localhost:5432/stocknow

# Redis (ìºì‹±)
REDIS_URL=redis://localhost:6379/0

# API í‚¤
ALPHA_VANTAGE_KEY=your_key_here
FINNHUB_API_KEY=your_key_here

# í¬ë¡¤ëŸ¬ ì„¤ì •
CRAWL_INTERVAL=300  # ì´ˆ
MAX_WORKERS=10
USER_AGENT=Mozilla/5.0...

# NLP ëª¨ë¸
SPACY_MODEL=en_core_web_sm
SENTIMENT_MODEL=ProsusAI/finbert
```

---

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”

### 1. ìºì‹±
```python
from functools import lru_cache
import redis

redis_client = redis.from_url(REDIS_URL)

@lru_cache(maxsize=1000)
def get_ticker_info(symbol: str):
    # ìºì‹œëœ í‹°ì»¤ ì •ë³´ ë°˜í™˜
    pass
```

### 2. ë¹„ë™ê¸° í¬ë¡¤ë§
```python
import asyncio
import aiohttp

async def fetch_multiple_sources():
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_source(session, url) for url in urls]
        return await asyncio.gather(*tasks)
```

### 3. ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ì‹±
```sql
CREATE INDEX idx_composite ON news_articles(published_at, sentiment_score);
CREATE INDEX idx_ticker_news ON news_tickers(ticker_symbol, created_at);
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸

```bash
# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
pytest tests/unit/

# í†µí•© í…ŒìŠ¤íŠ¸
pytest tests/integration/

# ì»¤ë²„ë¦¬ì§€ í™•ì¸
pytest --cov=app tests/
```

---

## ğŸ“ ë¼ì´ì„¼ìŠ¤

MIT License

---

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“§ ë¬¸ì˜

ë¬¸ì œê°€ ë°œìƒí•˜ë©´ Issues íƒ­ì— ë“±ë¡í•´ì£¼ì„¸ìš”.